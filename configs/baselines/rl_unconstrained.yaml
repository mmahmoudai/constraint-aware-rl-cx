# Baseline: RL Augmentation (Unconstrained)
# PPO agent without clinical parameter bounds or calibration reward
# Accuracy-only optimization to isolate the contribution of constraint-aware design
experiment_name: "rl_unconstrained"
seed: 42
data_root: "/data"
output_dir: "outputs"
num_workers: 8
epochs: 150
warmup_epochs: 5
mc_dropout_passes: 20
use_rl: true

classifier:
  d_model: 64
  nhead: 4
  dim_ff: 256
  n_layers: 2
  dropout: 0.3
  lr: 2.0e-4
  weight_decay: 0.01
  batch_size: 64
  use_mixstyle: false

ppo:
  state_dim: 100
  n_actions: 60
  hidden_dim: 256
  clip_eps: 0.2
  entropy_coeff: 0.02
  gamma: 0.99
  gae_lambda: 0.95
  actor_lr: 1.0e-4
  critic_lr: 5.0e-4
  buffer_size: 1024
  batch_size: 128

augmentation:
  type: "rl"
  constrained: false    # no clinical bounds
  use_curriculum: true

curriculum:
  n_stages: 5
  rollback_thresh: 0.02
  tau_adapt: 100.0

# Accuracy-only reward (no calibration, no implausibility penalty)
reward:
  alpha: 1.0
  beta: 0.3
  delta: 0.0       # no calibration reward
  gamma_cost: 0.1
  epsilon: 0.0      # no implausibility penalty
