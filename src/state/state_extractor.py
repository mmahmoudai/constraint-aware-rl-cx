"""
Modality-aware state representation extractor.

Produces a 100-dimensional state vector composed of four groups:
    1. Global image statistics (32 dims): mean intensity, contrast, histogram
       features, edge density, spatial frequency, percentile values.
    2. Lung field characteristics (28 dims): area ratios, centroids, aspect
       ratios, boundary smoothness, symmetry from a U-Net segmentation model.
    3. Uncertainty features (24 dims): MC Dropout confidence histograms (8 bins),
       mean/variance of predictive entropy, per-class uncertainty.
    4. Training dynamics (16 dims): loss trajectory (last 5 steps), gradient norm,
       learning rate, epoch fraction, curriculum stage, EMA of validation accuracy.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import cv2


class LungSegmentationUNet(nn.Module):
    """Lightweight U-Net for lung field segmentation (3.1M parameters).

    Pretrained on JSRT and Montgomery County CXR datasets.
    Dice = 0.96 on adult CXRs, ~0.91 on pediatric images.
    """

    def __init__(self):
        super().__init__()
        # encoder
        self.enc1 = self._block(1, 32)
        self.enc2 = self._block(32, 64)
        self.enc3 = self._block(64, 128)
        self.enc4 = self._block(128, 256)

        self.pool = nn.MaxPool2d(2)

        # bottleneck
        self.bottleneck = self._block(256, 512)

        # decoder
        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec4 = self._block(512, 256)
        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec3 = self._block(256, 128)
        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec2 = self._block(128, 64)
        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)
        self.dec1 = self._block(64, 32)

        self.out_conv = nn.Conv2d(32, 1, kernel_size=1)

    @staticmethod
    def _block(in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        # x: (B, 1, H, W)
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))

        b = self.bottleneck(self.pool(e4))

        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))
        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))
        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))
        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))

        return torch.sigmoid(self.out_conv(d1))


class StateExtractor:
    """Extracts the 100-dimensional modality-aware state vector.

    Args:
        seg_model:   pretrained LungSegmentationUNet (or None to skip lung features)
        classifier:  CNN-Transformer classifier (for uncertainty features)
        device:      torch device
        n_mc:        number of MC Dropout passes for uncertainty (default 20)
    """

    def __init__(
        self,
        seg_model=None,
        classifier=None,
        device: torch.device = torch.device("cpu"),
        n_mc: int = 20,
    ):
        self.seg_model = seg_model
        self.classifier = classifier
        self.device = device
        self.n_mc = n_mc

        # training dynamics tracker
        self._loss_history = []
        self._grad_norm = 0.0
        self._lr = 0.0
        self._epoch_fraction = 0.0
        self._curriculum_stage = 0
        self._val_acc_ema = 0.5

    def extract(self, image: torch.Tensor) -> torch.Tensor:
        """Extract full 100-dim state vector for a single image.

        Args:
            image: (C, H, W) or (1, C, H, W) tensor, normalized

        Returns:
            (100,) state vector
        """
        if image.dim() == 3:
            image = image.unsqueeze(0)

        image = image.to(self.device)
        parts = []

        # 1. Global image statistics (32 dims)
        parts.append(self._image_statistics(image))

        # 2. Lung field characteristics (28 dims)
        parts.append(self._lung_features(image))

        # 3. Uncertainty features (24 dims)
        parts.append(self._uncertainty_features(image))

        # 4. Training dynamics (16 dims)
        parts.append(self._training_dynamics())

        state = torch.cat(parts, dim=-1)
        assert state.shape[-1] == 100, f"State dim {state.shape[-1]} != 100"
        return state.squeeze(0)

    def extract_batch(self, images: torch.Tensor) -> torch.Tensor:
        """Extract state vectors for a batch of images.

        Args:
            images: (B, C, H, W) tensor

        Returns:
            (B, 100) state tensor
        """
        states = []
        for i in range(images.size(0)):
            states.append(self.extract(images[i]))
        return torch.stack(states)

    def _image_statistics(self, image: torch.Tensor) -> torch.Tensor:
        """Global image statistics (32 dims)."""
        B = image.size(0)
        # use grayscale channel
        gray = image[:, 0:1, :, :]  # (B, 1, H, W)
        flat = gray.flatten(1)  # (B, H*W)

        feats = []
        # mean intensity (1)
        feats.append(flat.mean(dim=1, keepdim=True))
        # std / contrast (1)
        feats.append(flat.std(dim=1, keepdim=True))
        # histogram features: 10 bins (10)
        for i in range(10):
            lo = i / 10.0
            hi = (i + 1) / 10.0
            mask = ((flat >= lo) & (flat < hi)).float()
            feats.append(mask.mean(dim=1, keepdim=True))
        # edge density via Sobel approximation (1)
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],
                               dtype=torch.float32, device=self.device).view(1, 1, 3, 3)
        sobel_y = sobel_x.transpose(2, 3)
        edges_x = F.conv2d(gray, sobel_x, padding=1)
        edges_y = F.conv2d(gray, sobel_y, padding=1)
        edge_mag = (edges_x ** 2 + edges_y ** 2).sqrt()
        feats.append(edge_mag.mean(dim=[1, 2, 3]).unsqueeze(1))
        # spatial frequency: mean of FFT magnitude (1)
        fft = torch.fft.fft2(gray)
        fft_mag = torch.abs(fft).mean(dim=[1, 2, 3]).unsqueeze(1)
        feats.append(fft_mag)
        # percentile values: 5th, 25th, 50th, 75th, 95th (5)
        for q in [0.05, 0.25, 0.50, 0.75, 0.95]:
            feats.append(torch.quantile(flat, q, dim=1, keepdim=True))
        # skewness and kurtosis (2)
        mean = flat.mean(dim=1, keepdim=True)
        std = flat.std(dim=1, keepdim=True) + 1e-8
        z = (flat - mean) / std
        feats.append(z.pow(3).mean(dim=1, keepdim=True))  # skewness
        feats.append(z.pow(4).mean(dim=1, keepdim=True))  # kurtosis

        result = torch.cat(feats, dim=1)  # (B, 22) so far
        # pad to 32 dims with zeros
        pad_size = 32 - result.size(1)
        if pad_size > 0:
            result = torch.cat([result, torch.zeros(B, pad_size, device=self.device)], dim=1)
        return result[:, :32]

    def _lung_features(self, image: torch.Tensor) -> torch.Tensor:
        """Lung field characteristics from segmentation model (28 dims)."""
        B = image.size(0)
        if self.seg_model is None:
            return torch.zeros(B, 28, device=self.device)

        with torch.no_grad():
            gray = image[:, 0:1, :, :]
            mask = self.seg_model(gray)  # (B, 1, H, W)
            mask_bin = (mask > 0.5).float()

        feats = []
        for b in range(B):
            m = mask_bin[b, 0].cpu().numpy()
            feats.append(self._compute_lung_features_single(m))

        return torch.tensor(np.stack(feats), dtype=torch.float32, device=self.device)

    def _compute_lung_features_single(self, mask: np.ndarray) -> np.ndarray:
        """Compute 28-dim lung features from a binary mask."""
        H, W = mask.shape
        total_pixels = H * W
        features = np.zeros(28)

        # area ratio (1)
        area = mask.sum()
        features[0] = area / total_pixels

        if area < 10:
            return features

        # find left and right lung via connected components
        mask_uint8 = (mask * 255).astype(np.uint8)
        n_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
            mask_uint8, connectivity=8
        )

        if n_labels < 2:
            return features

        # sort components by area (skip background=0)
        comp_areas = [(i, stats[i, cv2.CC_STAT_AREA]) for i in range(1, n_labels)]
        comp_areas.sort(key=lambda x: x[1], reverse=True)

        # take up to 2 largest components (left and right lung)
        for idx, (comp_id, comp_area) in enumerate(comp_areas[:2]):
            offset = idx * 12
            cx, cy = centroids[comp_id]
            x, y = stats[comp_id, cv2.CC_STAT_LEFT], stats[comp_id, cv2.CC_STAT_TOP]
            w, h = stats[comp_id, cv2.CC_STAT_WIDTH], stats[comp_id, cv2.CC_STAT_HEIGHT]

            features[1 + offset] = comp_area / total_pixels    # area ratio
            features[2 + offset] = cx / W                       # centroid x
            features[3 + offset] = cy / H                       # centroid y
            features[4 + offset] = w / h if h > 0 else 0        # aspect ratio
            features[5 + offset] = w / W                         # width ratio
            features[6 + offset] = h / H                         # height ratio

            # boundary smoothness: perimeter / sqrt(area)
            comp_mask = (labels == comp_id).astype(np.uint8)
            contours, _ = cv2.findContours(comp_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if contours:
                perimeter = cv2.arcLength(contours[0], True)
                features[7 + offset] = perimeter / (np.sqrt(comp_area) + 1e-6)

        # symmetry: area ratio between two largest components (1)
        if len(comp_areas) >= 2:
            a1, a2 = comp_areas[0][1], comp_areas[1][1]
            features[25] = min(a1, a2) / (max(a1, a2) + 1e-6)

        # centroid symmetry: horizontal distance between centroids (1)
        if len(comp_areas) >= 2:
            cx1 = centroids[comp_areas[0][0]][0]
            cx2 = centroids[comp_areas[1][0]][0]
            features[26] = abs(cx1 - cx2) / W

        # total lung area fraction (1)
        features[27] = area / total_pixels

        return features

    def _uncertainty_features(self, image: torch.Tensor) -> torch.Tensor:
        """MC Dropout uncertainty features (24 dims)."""
        B = image.size(0)
        if self.classifier is None:
            return torch.zeros(B, 24, device=self.device)

        unc = self.classifier.mc_dropout_uncertainty(image, n_forward=self.n_mc)

        feats = []
        # confidence histograms (8 bins)
        feats.append(unc["confidence_hist"])  # (B, 8)
        # mean and variance of predictive entropy (2)
        feats.append(unc["entropy"].unsqueeze(1))
        feats.append(unc["entropy"].pow(2).unsqueeze(1))
        # per-class uncertainty: variance for each class (2 Ã— 2 = 4)
        feats.append(unc["variance"])  # (B, 2)
        feats.append(unc["mean_probs"])  # (B, 2)
        # mean confidence and variance across passes (2)
        feats.append(unc["mean_probs"].max(dim=1, keepdim=True).values)
        feats.append(unc["variance"].mean(dim=1, keepdim=True))

        result = torch.cat(feats, dim=1)  # (B, ~17)
        # pad to 24 dims
        pad_size = 24 - result.size(1)
        if pad_size > 0:
            result = torch.cat([result, torch.zeros(B, pad_size, device=self.device)], dim=1)
        return result[:, :24]

    def _training_dynamics(self) -> torch.Tensor:
        """Training dynamics features (16 dims)."""
        feats = np.zeros(16)

        # loss trajectory: last 5 steps (5)
        recent = self._loss_history[-5:] if self._loss_history else [0.0]
        for i, v in enumerate(recent):
            if i < 5:
                feats[i] = v

        # gradient norm (1)
        feats[5] = self._grad_norm
        # learning rate (1)
        feats[6] = self._lr
        # epoch fraction (1)
        feats[7] = self._epoch_fraction
        # curriculum stage (normalized) (1)
        feats[8] = self._curriculum_stage / 4.0
        # EMA of validation accuracy (1)
        feats[9] = self._val_acc_ema

        # loss trend: slope of last 5 losses (1)
        if len(self._loss_history) >= 2:
            recent_losses = self._loss_history[-5:]
            x = np.arange(len(recent_losses))
            if len(x) > 1:
                slope = np.polyfit(x, recent_losses, 1)[0]
                feats[10] = slope

        # loss variance (1)
        if len(self._loss_history) >= 2:
            feats[11] = np.var(self._loss_history[-10:])

        # remaining 4 dims reserved for future features
        return torch.tensor(feats, dtype=torch.float32, device=self.device).unsqueeze(0)

    def update_dynamics(
        self,
        loss: float,
        grad_norm: float,
        lr: float,
        epoch_fraction: float,
        curriculum_stage: int,
        val_acc_ema: float,
    ):
        """Update training dynamics features."""
        self._loss_history.append(loss)
        if len(self._loss_history) > 100:
            self._loss_history = self._loss_history[-100:]
        self._grad_norm = grad_norm
        self._lr = lr
        self._epoch_fraction = epoch_fraction
        self._curriculum_stage = curriculum_stage
        self._val_acc_ema = val_acc_ema
